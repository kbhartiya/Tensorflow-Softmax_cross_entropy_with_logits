# Tensorflow-Softmax_cross_entropy_with_logits
This is the implementation of tf.nn.softmax_cross_entropy_with_logits, a function in tensoflow to compute the Cross_entropy loss with softmax classifier.
![alt tensorflow](https://github.com/kbhartiya83/Tensorflow-Softmax_cross_entropy_with_logits/blob/master/2000px-TensorFlowLogo.svg.png)
### Official Documentation for tf.nn.softmax_cross_entropy_with_logits of tensorflow.
https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits
## Softmax Function
The softmax function, or normalised exponential function is a generalisation of logistic function that "squashes" a K-dimensional vector {\displaystyle \mathbf {z} } \mathbf {z}  of arbitrary real values to a K-dimensional vector {\displaystyle \sigma (\mathbf {z} )} \sigma (\mathbf {z} ) of real values, where each entry is in the range (0, 1), and all the entries add up to 1.
