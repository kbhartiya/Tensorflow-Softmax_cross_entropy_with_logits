> # Softmax_cross_entropy_with_logits
- This is the implementation of tf.nn.softmax_cross_entropy_with_logits, a function in tensoflow to compute the Cross_entropy loss with softmax classifier.
![alt tensorflow](https://github.com/kbhartiya83/Tensorflow-Softmax_cross_entropy_with_logits/blob/master/2000px-TensorFlowLogo.svg.png)
### Official Documentation for tf.nn.softmax_cross_entropy_with_logits of tensorflow.
- https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits
> ## Softmax Function

- The softmax function, or normalised exponential function is a generalisation of logistic function that "squashes" a K-dimensional vector z of arbitrary real values to a K-dimensional vector of real values, where each entry is in the range (0, 1), and all the entries add up to 1.
![alt Softmax Function](https://github.com/kbhartiya83/Tensorflow-Softmax_cross_entropy_with_logits/blob/master/1_vz9WitVXiK3KM28n9JgTgw%402x.png)


- In probability theory, the output of the softmax function can be used to represent a categorical distribution â€“ that is, a probability distribution over K different possible outcomes.

> ## Logits.
- Logits are functions that map probabilities. It maps probabiltity score from [0, 1] to (-inf, inf)
![alt Logits](https://github.com/kbhartiya/Tensorflow-Softmax_cross_entropy_with_logits/blob/master/zto5q.png)
